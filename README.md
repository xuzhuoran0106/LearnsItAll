# LearnsItAll
This repo aims to train a deep learning model that learns lifelong, and has infinite possibility to improve itself even may overtake its creator. 
# Train lifelong learning model
Deep learning has a training phase and an inference phase. Feedback signal is provided only at training phase. So learning only happens in the training phase using algorithm like backpropagation. 

But for lifelong learning, feedback signal is provided at both training phase and inference phase. So learning can happen in both training phase and inference phase. Algorithms like backpropagation are used in training phase to learn both knowledge about specific task and some learning skill. The learned learning skill is used in the inference phase to update itself according to feedback signal. Not only task specific knowledge, but even learning skill can be improved in the inference phase .

Such a lifelong learning problem can be defined as below:
```
while not end:
  Take a sample (ğ¼,ğºğ‘‡)
  for i in [0,...,n]:
    ğ‘‚_ğ‘–=ğ¿ğ‘†ğ‘‡ğ‘€(ğ¼,ğ‘ğ‘œğ‘›ğ‘’)
    ğ¿ğ‘†ğ‘‡ğ‘€(ğ‘ğ‘œğ‘›ğ‘’,ğºğ‘‡)
    ğ¿ğ‘œğ‘ ğ‘ _ğ‘–=ğ‘™ğ‘œğ‘ ğ‘ (ğ‘‚_ğ‘–,ğºğ‘‡)
Using BP to maximize: ğ¿ğ‘œğ‘ ğ‘ _ğ‘›âˆ’ğ¿ğ‘œğ‘ ğ‘ _0
```
We use LSTM here because it has potential ability to learn everything. We will explain the reason later.

# A model with infinite talent
Considering a model has infinite talent to learn everything, three fundamental requirements should be meet:
1. The model can read every parts of its knowledge.
2. The model can convert computing result as new knowledge.
3. The model can modify every parts of its knowledge. 

Such a model can be defined as below:
```
The model is a time series model. ğ¼_ğ‘¡ is the input at time step ğ‘¡. ğºğ‘‡_ğ‘¡ is the feedback at time step ğ‘¡.

Compute Step:
ğ‘œ_ğ‘¡^1=ğ‘“_1 (ğ¼_ğ‘¡,ğºğ‘‡_ğ‘¡,â„_(ğ‘¡âˆ’1);ğ‘¤_(ğ‘¡âˆ’1)^1)
ğ‘œ_ğ‘¡^2=ğ‘“_2 (ğ¼_ğ‘¡,ğºğ‘‡_ğ‘¡,â„_(ğ‘¡âˆ’1);ğ‘¤_(ğ‘¡âˆ’1)^2)
â€¦
ğ‘œ_ğ‘¡^ğ‘›=ğ‘“_ğ‘› (ğ¼_ğ‘¡,ğºğ‘‡_ğ‘¡,â„_(ğ‘¡âˆ’1);ğ‘¤_(ğ‘¡âˆ’1)^ğ‘›)


Weights Update:
ğ‘_ğ‘¡^(ğ‘–,0),ğ‘_ğ‘¡^(ğ‘–,1),â€¦,ğ‘_ğ‘¡^(ğ‘–,ğ‘›)= ğ‘†_ğ‘¤(ğ‘œ_ğ‘¡^ğ‘–;ğ‘œ_ğ‘¡^1,â€¦,ğ‘œ_ğ‘¡^ğ‘›);      ğ‘†_ğ‘¤ is a non-parameter function that satisfies âˆ‘ğ‘_ğ‘¡^(ğ‘–,0),ğ‘_ğ‘¡^(ğ‘–,1),â€¦,ğ‘_ğ‘¡^(ğ‘–,ğ‘›)=1
ğ‘¤_ğ‘¡^1=ğ‘_ğ‘¡^1,0 ğ‘¤_ğ‘¡^1+ğ‘_ğ‘¡^1,1 ğ‘œ_ğ‘¡^1+â€¦ğ‘_ğ‘¡^(1,ğ‘›)ğ‘œ_ğ‘¡^ğ‘›
â€¦
ğ‘¤_ğ‘¡^ğ‘›=ğ‘_ğ‘¡^(ğ‘›,0) ğ‘¤_ğ‘¡^ğ‘›+ğ‘_ğ‘¡^(ğ‘›,1) ğ‘œ_ğ‘¡^1+â€¦ğ‘_ğ‘¡^(ğ‘›,ğ‘›) ğ‘œ_ğ‘¡^ğ‘›

Hidden States Update:
ğ‘_ğ‘¡^(1_ğ‘œ),ğ‘_ğ‘¡^(1_ğ‘¤)â€¦,ğ‘_ğ‘¡^(ğ‘›_ğ‘œ),ğ‘_ğ‘¡^(ğ‘›_ğ‘¤)=ğ‘†_â„(ğ‘œ_ğ‘¡^1,â€¦,ğ‘œ_ğ‘¡^ğ‘›);   ğ‘†_â„ is a non-parameter function that satisfies âˆ‘ğ‘_ğ‘¡^(1_ğ‘œ),ğ‘_ğ‘¡^(1_ğ‘¤),â€¦,ğ‘_ğ‘¡^(ğ‘›_ğ‘œ),ğ‘_ğ‘¡^(ğ‘›_ğ‘¤)=1
â„_ğ‘¡=ğ‘_ğ‘¡^(1_ğ‘œ)ğ‘œ_ğ‘¡^1+ğ‘_ğ‘¡^(1_ğ‘¤)ğ‘¤_ğ‘¡^1+â€¦+ğ‘_ğ‘¡^(ğ‘›_ğ‘œ)ğ‘œ_ğ‘¡^ğ‘›+ğ‘_ğ‘¡^(ğ‘›_ğ‘¤)ğ‘¤_ğ‘¡^ğ‘›
```

We find this model is equivalent to a LSTM model.
First we notice that Eq.1 is equivalent to Eq.2. 
```
y_0=ğ‘¤_0ğ‘¥_0
ğ‘¤_1=ğ‘¤_0+ğ‘¦_0
ğ‘¦_1=ğ‘¤_1ğ‘¥_1
Eq.1 
```
```
ğ‘¦_1=(ğ‘¤_0+ğ‘¤_0ğ‘¥_0)ğ‘¥_1
Eq.2
```
Next we notice that Eq.3 is equivalent to Eq.2 and compatible with deep learning frameworks. Hence, we can use hidden state to replace the weights update. We also notice that the hidden state naturally satisfies the three requirements mentioned above.  
```
â„_ğ‘ =0 
h_0=(ğ‘¤_0+â„_ğ‘ )ğ‘¥_0=(ğ‘¤_0+â„_ğ‘ )ğ‘¥_0=ğ‘¤_0ğ‘¥_0
â„_1=(ğ‘¤_0+â„_0)ğ‘¥_1= (ğ‘¤_0+(ğ‘¤_0+â„_ğ‘ )ğ‘¥_0)ğ‘¥_1=(ğ‘¤_0+ğ‘¤_0ğ‘¥_0)ğ‘¥_1
Eq.3
```
Finally Eq.3 can be rewrote as Eq.4. It shows the computation of â„ğ‘¥ is independent to ğ‘¤ğ‘¥. The form of Eq.4 is similar to a lot of existing models, such as LSTM. 
```
â„_ğ‘ =0 
h_0=(ğ‘¤_0+â„_ğ‘ )ğ‘¥_0=ğ‘¤_0ğ‘¥_0+â„_ğ‘ ğ‘¥_0
â„_1=(ğ‘¤_0+â„_0)ğ‘¥_1=ğ‘¤_0ğ‘¥_1+h_0ğ‘¥_1
Eq.4
```
So a simple LSTM has infinite talent to learn everything. The weights fixed at training phase of LSTM can be considered as instinct. The hidden states at inference phase can be considered as both task specific knowledge and learning skills.


